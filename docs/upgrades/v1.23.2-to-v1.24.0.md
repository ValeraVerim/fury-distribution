# Kubernetes Fury Distribution v1.23.2 to 1.24.0 Upgrade Guide

This guide describes the steps to follow to upgrade the Kubernetes Fury Distribution (KFD) from v1.23.2 to v1.24.0. If you are running a custom set of modules, or different versions than the ones included with each release of KFD, please refer to each module's release notes.

Notice that the guide will not cover cloud related changes, ingresses or pod placement changes. Only changes related to KFD and its modules.

> ‚ö†Ô∏è **WARNING**
> The upgrade process involves downtime of some components.

## Upgrade procedure

As a high-level overview, the upgrade procedure consists on:

1. Upgrading KFD (all the core modules).
2. Upgrading the Kubernetes cluster itself.

### 1. Upgrade KFD

The suggested approach to upgrade the distribution is to do it one module at a time, to reduce the risk of errors and to make the process more manageable.

#### Networking module upgrade

To upgrade the Networking module to the new version, update the version on the `Furyfile.yml` file to the new version:

```yaml
versions:
  networking: v1.10.0
...
```

Then, download the new modules with `furyctl` with the following command:

```bash
furyctl vendor -H
```

Apply your Kustomize project that uses Networking module packages as bases with:

```bash
kustomize build <your-project-path> | kubectl apply -f - --server-side --force-conflicts
```

#### Monitoring module upgrade

To upgrade the Monitoring module to the new version, update the version on the `Furyfile.yml` file to the new version:

```yaml
versions:
...
  monitoring: v2.0.0
...
```

Then, download the new modules with `furyctl` with the following command:

```bash
furyctl vendor -H
```

This time, before applying the project, you need to do some manual steps on the existing resources:

> ‚ö†Ô∏è some downtime of the Monitoring stack is expected during this process.

```bash
kubectl delete deployments.apps prometheus-operator -n monitoring

kubectl delete poddisruptionbudgets.policy prometheus-k8s -n monitoring
kubectl delete clusterrolebinding.rbac.authorization.k8s.io prometheus-k8s-scrape
kubectl delete clusterroles.rbac.authorization.k8s.io prometheus-k8s-scrape
kubectl delete prometheusrules.monitoring.coreos.com prometheus-k8s-rules -n monitoring

kubectl delete poddisruptionbudget.policy alertmanager-main -n monitoring

# Clean up goldpinger
kubectl delete servicemonitor.monitoring.coreos.com goldpinger -n monitoring
kubectl delete service goldpinger -n monitoring
kubectl delete daemonset.apps goldpinger -n monitoring
kubectl delete clusterrole.rbac.authorization.k8s.io goldpinger
kubectl delete serviceaccount goldpinger -n monitoring
kubectl delete rolebinding.rbac.authorization.k8s.io goldpinger:cluster:view -n monitoring

kubectl delete deployments.apps grafana -n monitoring

kubectl delete deployments.apps kube-state-metrics -n monitoring

# Clean up metrics-server
kubectl delete apiservice.apiregistration.k8s.io v1beta1.metrics.k8s.io
kubectl delete service metrics-server -n kube-system
kubectl delete deployment.apps metrics-server -n kube-system
kubectl delete clusterrolebinding.rbac.authorization.k8s.io metrics-server:system:auth-delegator
kubectl delete clusterrolebinding.rbac.authorization.k8s.io system:metrics-server
kubectl delete clusterrole.rbac.authorization.k8s.io system:aggregated-metrics-reader
kubectl delete clusterrole.rbac.authorization.k8s.io system:metrics-server
kubectl delete rolebinding.rbac.authorization.k8s.io metrics-server-auth-reader -n kube-system
kubectl delete serviceaccount metrics-server -n kube-system
kubectl delete certificate.cert-manager.io metrics-server-tls -n kube-system
kubectl delete certificate.cert-manager.io metrics-server-ca -n kube-system
kubectl delete issuer.cert-manager.io metrics-server-ca -n kube-system
kubectl delete issuer.cert-manager.io metrics-server-selfsign -n kube-system

kubectl delete daemonsets.apps node-exporter -n monitoring

kubectl delete serviceaccount x509-certificate-exporter-node -n monitoring
kubectl delete clusterrole.rbac.authorization.k8s.io x509-certificate-exporter-node
kubectl delete clusterrolebinding.rbac.authorization.k8s.io x509-certificate-exporter-node
kubectl delete daemonset.apps x509-certificate-exporter-nodes -n monitoring
```

Replace `metrics-server` with `prometheus-adapter` package as a base to your project, to replace the functionalities provided by `metrics-server`.

Then apply your Kustomize project that uses Monitoring module packages as bases with:

```bash
kustomize build <your-project-path> | kubectl apply -f - --server-side --force-conflicts
```

#### Logging module upgrade

To upgrade the Logging module to the new version, update the version on the `Furyfile.yml` file to the new version:

```yaml
versions:
...
  logging: v3.0.1
...
```

Then, download the new modules with `furyctl` with the following command:

```bash
furyctl vendor -H
```

Since this upgrade is a major one, there are some manual steps involving breaking changes that you need to do before applying the project:

> ‚ö†Ô∏è some downtime of the Logging stack is expected during this process.

Remove the old fluentd and fluentbit stack:

```bash
kubectl delete ds fluentbit -n logging
kubectl delete sts fluentd -n logging
```

Remove `fluentd`, `elasticsearch-{single,triple}` and `kibana` from your kustomize project and replace them with
`logging-operator`, `logging-operated`,`opensearch-{single,triple}`, `opensearch-dashboards`, `configs` bases on your custom kustomize project.

Apply your Kustomize project that uses Logging module packages as bases with:

```bash
kustomize build <your-project-path> | kubectl apply -f - --server-side --force-conflicts
```

All the logs will now flow to the new OpenSearch stack.

You can leave the old Elasticsearch/Kibana stack running and remove it after you've verified that everything is working as expected and you don't need the data stored in ElasticSearch anymore. To do so, run the following commands:

```bash
kubectl delete statefulset elasticsearch -n logging
kubectl delete service elasticsearch -n logging
kubectl delete prometheusrule es-rules -n logging
kubectl delete servicemonitor elasticsearch -n logging
kubectl delete deployment kibana -n logging
kubectl delete service kibana -n logging
kubectl delete cronjob curator -n logging
```

> üí° we recommend leaving the ElasticSearch/Kibana stack up for a breif period (like 30 days) and then proceed to delete it.

#### Ingress module upgrade

To upgrade the Ingress module to the new version, update the version on the `Furyfile.yml` file to the new version:

```yaml
versions:
...
  ingress: v1.13.0
...
```

Then, download the new modules with `furyctl` with the following command:

```bash
furyctl vendor -H
```

If you are using the *dual* NGINX Ingress Controller package, make sure that all your ingresses have the `.spec.ingressClass` field set and that they **don't have** the `kubernetes.io/ingress.class` annotation before proceeding.

cert-manager has been bumped several versions, please check the upgrade guides in the official documentation. In particular, the update from v1.7 to v1.8 includes some changes to the `spec.privateKey.rotationPolicy` field, read carefuly if you were using it or you had the `--feature-gates=ServerSideApply=true` flag in the cert-manager controller.

Here you can find the relevant upgrade docs:

- <https://cert-manager.io/docs/installation/upgrading/upgrading-1.6-1.7>
- <https://cert-manager.io/docs/installation/upgrading/upgrading-1.7-1.8>
- <https://cert-manager.io/docs/installation/upgrading/upgrading-1.8-1.9>
- <https://cert-manager.io/docs/installation/upgrading/upgrading-1.9-1.10>

Apply your Kustomize project that uses Ingress module packages as bases with:

> ‚ö†Ô∏è some downtime of the NGINX Ingress Controller is expected during the upgrade process.

```bash
# For NGINX Ingress Controller SINGLE
kubectl delete ingressclass nginx -n ingress-nginx
# For NGINX Ingress Controller DUAL
kubectl delete ingressclass external internal -n ingress-nginx
# finally
kustomize build <your-project-path> | kubectl apply -f - --server-side --force-conflicts
```

#### Disaster Recovery module upgrade

To upgrade the Disaster Recovery module to the new version, update the version on the `Furyfile.yml` file to the new version:

```yaml
versions:
...
  dr: v1.10.0
...
```

Then, download the new modules with `furyctl` with the following command:

```bash
furyctl vendor -H
```

Apply your Kustomize project that uses Ingress module packages as bases with:

```bash
kustomize build <your-project-path> | kubectl apply -f - --server-side --force-conflicts
```

> Notice that `velero-eks` has been deprecated, please use the new `aws-velero` terraform module instead in case you haven't migrated yet.

#### OPA module upgrade

To upgrade the OPA module to the new version, update the version on the `Furyfile.yml` file to the new version:

> ‚ö†Ô∏è notice that the `http.send` OPA built-in is disabled. [Read here for more details](https://open-policy-agent.github.io/gatekeeper/website/docs/externaldata#motivation).

```yaml
versions:
...
  opa: v1.7.2
...
```

Then, download the new modules with `furyctl` with the following command:

```bash
furyctl vendor -H
```

Apply your Kustomize project that uses OPA module packages as bases with:

```bash
kustomize build <your-project-path> | kubectl apply -f - --server-side --force-conflicts
```

#### Auth module upgrade

The Auth module is a new addition to KFD, there is no previous version to upgrade from, but, you could have been using Pomerium and/or Dex and Gangway which were previously included in the Ingress and on-premises modules respectively.

> üí° Pomerium is at the same previously included version and Dex has been updated for compatibility with Kubernetes 1.24.x, there are no breaking changes.

If you were using these components, adjust your Kustomize project to use the new `auth` module as a base:

```yaml
versions:
...
  auth: v0.0.2
...
```

Then, download the new modules with `furyctl` with the following command:

```bash
furyctl vendor -H
```

> üí° Be sure to enable the `customHTMLTemplatesDir: /custom-templates` config option in Gangway's configuration to use the Fury branded templates.
> See the [example configuration file](https://github.com/sighupio/fury-kubernetes-auth/blob/33ac4818232a155ee3920cfabf1b3eb2a9720e7f/katalog/gangway/example/gangway.yml#L73).

Apply your Kustomize project that uses Auth module packages as bases with:

```bash
kustomize build <your-project-path> | kubectl apply -f - --server-side --force-conflicts
```

üéâ **CONGRATULATIONS** you have now successfully updated to KFD 1.24.0 üéâ

### 2. Upgrade Kubernetes

Being that the underlying Kubernetes cluster could have been created in several different ways, the upgrade of Kubernetes itself is considered out of the scope of this guide.

Please refer to the corresponding documentation for upgrade instructions.

For clusters created with Furyctl:

- [EKS Installer](https://github.com/sighupio/fury-eks-installer)
- [GKE Installer](https://github.com/sighupio/fury-gke-installer)
- [AKS Installer](https://github.com/sighupio/fury-aks-installer)

For clusters created with Fury on-premises: <https://github.com/sighupio/fury-kubernetes-on-premises/tree/main/examples/playbooks#upgrade-cluster>
