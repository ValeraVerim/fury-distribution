# Upgrade path from v1.23.2 to 1.24.0

This document describes the upgrade path from Fury distribution v1.23.2 to v1.24.0.

We will not cover cloud related changes, ingresses or pod placement changes. Only the changes related to the Fury distribution and its modules.

> NB: There will be some downtime on some components during the upgrade process.

## Upgrade procedure

The upgrade procedure consists on the upgrade of all the core modules, and when the upgrade of the core modules is completed, the upgrade of the Kubernetes cluster itself.

### Upgrade Fury distribution

The suggested approach on the upgrade of the distribution is to upgrade one module at a time, to reduce the risk of errors and to make the process more manageable.

### Networking module upgrade

To upgrade the monitoring module to the new version, update the version on the `Furyfile.yml` file to the new version:

```yaml
versions:
  networking: v1.10.0
...
```

Then, download the new modules with `furyctl` with the following command:

```bash
furyctl vendor -H
```

Apply your kustomize project that uses networking modules package as bases with:

```bash
kustomize build <your-project-path> | kubectl apply -f - --server-side --force-conflicts
```

### Monitoring module upgrade

To upgrade the monitoring module to the new version, update the version on the `Furyfile.yml` file to the new version:

```yaml
versions:
...
  monitoring: v2.0.0
...
```

Then, download the new modules with `furyctl` with the following command:

```bash
furyctl vendor -H
```

This time, before applying the project, you need to do some manual steps on the existing resources:

```bash
kubectl delete deployments.apps prometheus-operator -n monitoring

kubectl delete poddisruptionbudgets.policy prometheus-k8s -n monitoring
kubectl delete clusterrolebinding.rbac.authorization.k8s.io prometheus-k8s-scrape
kubectl delete clusterroles.rbac.authorization.k8s.io prometheus-k8s-scrape
kubectl delete prometheusrules.monitoring.coreos.com prometheus-k8s-rules -n monitoring

kubectl delete poddisruptionbudget.policy alertmanager-main -n monitoring

# Clean up goldpinger
kubectl delete servicemonitor.monitoring.coreos.com goldpinger -n monitoring
kubectl delete service goldpinger -n monitoring
kubectl delete daemonset.apps goldpinger -n monitoring
kubectl delete clusterrole.rbac.authorization.k8s.io goldpinger
kubectl delete serviceaccount goldpinger -n monitoring
kubectl delete rolebinding.rbac.authorization.k8s.io goldpinger:cluster:view -n monitoring

kubectl delete deployments.apps grafana -n monitoring

kubectl delete deployments.apps kube-state-metrics -n monitoring

# Clean up metrics-server
kubectl delete apiservice.apiregistration.k8s.io v1beta1.metrics.k8s.io
kubectl delete service metrics-server -n kube-system
kubectl delete deployment.apps metrics-server -n kube-system
kubectl delete clusterrolebinding.rbac.authorization.k8s.io metrics-server:system:auth-delegator
kubectl delete clusterrolebinding.rbac.authorization.k8s.io system:metrics-server
kubectl delete clusterrole.rbac.authorization.k8s.io system:aggregated-metrics-reader
kubectl delete clusterrole.rbac.authorization.k8s.io system:metrics-server
kubectl delete rolebinding.rbac.authorization.k8s.io metrics-server-auth-reader -n kube-system
kubectl delete serviceaccount metrics-server -n kube-system
kubectl delete certificate.cert-manager.io metrics-server-tls -n kube-system
kubectl delete certificate.cert-manager.io metrics-server-ca -n kube-system
kubectl delete issuer.cert-manager.io metrics-server-ca -n kube-system
kubectl delete issuer.cert-manager.io metrics-server-selfsign -n kube-system

kubectl delete daemonsets.apps node-exporter -n monitoring

kubectl delete serviceaccount x509-certificate-exporter-node -n monitoring
kubectl delete clusterrole.rbac.authorization.k8s.io x509-certificate-exporter-node
kubectl delete clusterrolebinding.rbac.authorization.k8s.io x509-certificate-exporter-node
kubectl delete daemonset.apps x509-certificate-exporter-nodes -n monitoring
```

Replace `metrics-server` with `prometheus-adapter` package as a base to your project, to replace the functionalities provided by `metrics-server`.

Then apply your kustomize project that uses monitoring modules package as bases with:

```bash
kustomize build <your-project-path> | kubectl apply -f - --server-side --force-conflicts
```

# Logging module upgrade

To upgrade the monitoring module to the new version, update the version on the `Furyfile.yml` file to the new version:

```yaml
versions:
...
  logging: v3.0.1
...
```

Then, download the new modules with `furyctl` with the following command:

```bash
furyctl vendor -H
```

Since this upgrade is a major one, there are some manual steps involving breaking changes that you need to do before applying the project:

Remove the old fluentd and fluentbit stack:

```bash
kubectl delete ds fluentbit -n logging
kubectl delete sts fluentd -n logging
```

Remove `fluentd`, `elasticsearch-{single,triple}` and `kibana` from your kustomize project and replace them with
`logging-operator`, `logging-operated`,`opensearch-{single,triple}`, `opensearch-dashboards`, `configs` bases on your custom kustomize project.

Apply your kustomize project that uses logging modules package as bases with:

```bash
kustomize build <your-project-path> | kubectl apply -f - --server-side --force-conflicts
```

All the logs will now flow to the new Opensearch stack. You can decide later to remove the old Elasticsearch/Kibana stack when everything is working as expected.

```bash
kubectl delete statefulset elasticsearch -n logging
kubectl delete service elasticsearch -n logging
kubectl delete prometheusrule es-rules -n logging
kubectl delete servicemonitor elasticsearch -n logging
kubectl delete deployment kibana -n logging
kubectl delete service kibana -n logging
kubectl delete cronjob curator -n logging
```

# Ingress module upgrade

